{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "e48uT5XiI1pH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.options.display.max_colwidth = 50\n",
        "import warnings; \n",
        "warnings.simplefilter('ignore')\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
      ],
      "metadata": {
        "id": "3DEy3XF2IdkI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "READING FILES"
      ],
      "metadata": {
        "id": "meVv6QB9a4bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_df = pd.read_csv(\"\")\n",
        "\n",
        "print(review_df.info())"
      ],
      "metadata": {
        "id": "_QQw9LG-JCV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOOKING AT THE DATA & SPLITTINT IT"
      ],
      "metadata": {
        "id": "7mDfE_ZqawCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.head()"
      ],
      "metadata": {
        "id": "IVRNfYqlJQya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#negative review data\n",
        "neg_data = review_df.loc[review_df['rating'] < 3]\n",
        "neg_data.reset_index(drop=True, inplace=True)\n",
        "print(neg_data.info())"
      ],
      "metadata": {
        "id": "XhfPcnfbJZKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#positive review data\n",
        "pos_data = review_df.loc[review_df['rating'] > 3]\n",
        "pos_data.reset_index(drop=True, inplace=True)\n",
        "print(pos_data.info())"
      ],
      "metadata": {
        "id": "y_NfTBzpUmvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREPROCESSING"
      ],
      "metadata": {
        "id": "IVTmB93gahGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing sentences into a list of words and removing uncessary characters\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  \n",
        "\n",
        "neg_data['data_words'] = list(sent_to_words(neg_data['review-en']))\n",
        "\n",
        "#Stop word removal and tokenizing \n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend([\"throw\",\"application\",\"question\",\"ask\",\"answer\", \"YKS\", \"TYT\", \"LGS\", \"AYT\"])  #extending stop words\n",
        "\n",
        "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "    texts_out = []\n",
        "    nlp = spacy.load(\"en_core_web_sm\",disable=['parser', 'ner'])\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
        "    return texts_out\n",
        "\n",
        "neg_data['data_ready'] = process_words(neg_data.data_words)  "
      ],
      "metadata": {
        "id": "lgXxTeYcJoSQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA"
      ],
      "metadata": {
        "id": "yWN3IMQGbBnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dict\n",
        "id2word = corpora.Dictionary(neg_data.data_ready)\n",
        "#Corpus\n",
        "corpus = [id2word.doc2bow(text) for text in neg_data.data_ready]\n",
        "#model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics= 8,  #8 topics                           \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=10,  \n",
        "                                           passes=10,   \n",
        "                                           alpha='symmetric',\n",
        "                                           iterations=100,\n",
        "                                           per_word_topics=True)\n",
        "\n",
        "#print topics and keywords\n",
        "print(lda_model.print_topics())"
      ],
      "metadata": {
        "id": "6Z3fBYrNJ9lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Perplexity\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "#Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=neg_data.data_ready, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "id": "eYTUjXXEKFwT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb001157-809b-49bc-e583-77108ca8342f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Perplexity:  -6.308499112298337\n",
            "\n",
            "Coherence Score:  0.4633786311709297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOPIC KEYWORDS"
      ],
      "metadata": {
        "id": "orx7ZNYebQfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in lda_model.show_topics(formatted=False, num_words= 15):\n",
        "    print('Topic: {} --> Words: {}'.format(idx, '/'.join([w[0] for w in topic])))"
      ],
      "metadata": {
        "id": "n1sWCv0UKKtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the topic distribution\n",
        "\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "#Finding the dominant topics\n",
        "def topics_per_review(model, corpus, start=0, end=1):\n",
        "    corpus_sel = corpus[start:end]\n",
        "    dominant_topics = []\n",
        "    topic_percentages = []\n",
        "    for i, corp in enumerate(corpus_sel):\n",
        "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
        "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
        "        dominant_topics.append((i, dominant_topic))\n",
        "        topic_percentages.append(topic_percs)\n",
        "    return(dominant_topics, topic_percentages)\n",
        "\n",
        "dominant_topics, topic_percentages = topics_per_review(model=lda_model, corpus=corpus, end=-1) "
      ],
      "metadata": {
        "id": "ca4gTl-TKQOE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution of topics in reviews\n",
        "df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
        "dominant_topic_in_each_rev = df.groupby('Dominant_Topic').size()\n",
        "df_dominant_topic_in_each_rev = dominant_topic_in_each_rev.to_frame(name='count').reset_index()\n",
        "display(df_dominant_topic_in_each_rev)"
      ],
      "metadata": {
        "id": "ATmSCZYlKWFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# total distrubution \n",
        "topic_weightage_by_rev = pd.DataFrame([dict(t) for t in topic_percentages])\n",
        "df_topic_weightage_by_rev = topic_weightage_by_rev.sum().to_frame(name='count').reset_index()\n",
        "\n",
        "display(df_topic_weightage_by_rev)"
      ],
      "metadata": {
        "id": "KLxAjeJHKaG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots \n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "# Top  Keywords for topics\n",
        "topic_top_n_words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n",
        "                                 for j, (topic, wt) in enumerate(topics) if j < 5]  # for 5 key words\n",
        "\n",
        "df_top_n_words_stacked = pd.DataFrame(topic_top_n_words, columns=['topic_id', 'words'])\n",
        "df_top_n_words = df_top_n_words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
        "df_top_n_words.reset_index(level=0,inplace=True)\n",
        "\n",
        "# Plot\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 4), dpi=120, sharey=True)\n",
        "\n",
        "# Topic Distribution by Dominant Topics\n",
        "ax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_rev, width=.5, color='#9ECBEA')\n",
        "ax1.set_xticks(range(df_dominant_topic_in_each_rev.Dominant_Topic.unique().__len__()))\n",
        "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top_n_words.loc[df_top_n_words.topic_id==x, 'words'].values[0])\n",
        "ax1.tick_params(labelsize=4)\n",
        "ax1.set_title('Number of Negative Reviews by Dominant Topic - APPNAME GooglePlay', fontdict=dict(size=8))\n",
        "ax1.set_ylabel('Number of Reviews', fontsize = 6)\n",
        "ax1.set_ylim(0, 100)\n",
        "\n",
        "\n",
        "# Topic Distribution by Topic Weights\n",
        "ax2.bar(x='index', height='count', data=df_topic_weightage_by_rev, width=.5, color='#EADA9E')\n",
        "ax2.set_xticks(range(df_topic_weightage_by_rev.index.unique().__len__()))\n",
        "ax2.xaxis.set_major_formatter(tick_formatter)\n",
        "ax2.tick_params(labelsize=4)\n",
        "ax2.set_title('Number of Negative Reviews  by Topic Weightage - APPNAME GooglePlay', fontdict=dict(size=8))\n",
        "ax2.set_ylabel('Number of Review', fontsize = 6)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RaLWRND7KggR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}